《机器学习实战》


2. KNN  计算距离分类

 kNN中文又称为k-近邻算法，其基本思想是通过计算输入样本点和训练集样本点之前的这两个向量之前的距离，距离越近的话，说明其特征

 越靠近，通过取出其k个距离最近的样本点，然后计算这k个样本点中类别占比最大的类比以此来预测输入样本点(被测样本点)的类别


kNN的优势
kNN是ML里最简单，最基本的算法。
kNN不会受到差别特别大的样本中的特征元素的影响(对异常值不敏感)。因为采用了归一化技术
kNN的精度高
kNN的劣势
kNN算法时间复杂度较高，需要计算被测样本点和训练集中所有样本点的距离


3. Decision Tree: 按照计算的熵分类

决策树（Decision Tree）算法主要用来处理分类问题，
熵（entropy）指的是体系的混乱的程度
信息熵（香农熵）： 是一种信息的度量方式，表示信息的混乱程度，也就是说：信息越有序，信息熵越低。例如：火柴有序放在火柴盒里，熵值很低，相反，熵值很高。
信息增益： 在划分数据集前后信息发生的变化称为信息增益。

		检测数据集中的所有数据的分类标签是否相同:
		    If so return 类标签
		    Else:
		        寻找划分数据集的最好特征（划分之后信息熵最小，也就是信息增益最大的特征）
		        划分数据集
		        创建分支节点
		            for 每个划分的子集
		                调用函数 createBranch （创建分支的函数）并增加返回结果到分支节点中
		        return 分支节点

优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。
缺点：可能会产生过度匹配问题。
适用数据类型：数值型和标称型。







4. Naive Bayes: 计算条件概率，选择高概率的决策
5. Logistic Regression/逻辑回归：更具梯度上升的方法，拟合数据，分类数据
6. SVM:二值分类器：求解一个二次优化问题，来最大化分类间隔，产生一个二值决策结果 (SMO/Sequential Minimal Optimization 序列最小优化)
8. 回归：预测数值数据， 包括：线性回归（最小二乘法拟合参数），局部加权线性回归,岭回归
9. 树回归：CART/分类树回归，建立二叉树然后使用回归分类 classifciation and regression tree


